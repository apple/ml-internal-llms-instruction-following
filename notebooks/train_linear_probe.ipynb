{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# For licensing see accompanying LICENSE file.\n",
    "# Copyright (C) 2025 Apple Inc. All Rights Reserved.\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import nltk.data\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "sys.path.append(\"../../\")\n",
    "sys.path.insert(0, os.path.abspath('..'))\n",
    "\n",
    "import torch\n",
    "import torch as t\n",
    "import pickle\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readjsonl(datapath):\n",
    "    res = []\n",
    "    with open(datapath, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f.readlines():\n",
    "            res.append(json.loads(line))\n",
    "    return res\n",
    "\n",
    "# // Get all detailed instructions\n",
    "def get_inst_list(task_path_ifeval=\"ifeval_simple\"):\n",
    "    task_path_ifeval=\"../data/\"+task_path_ifeval+\".jsonl\"\n",
    "    ifeval_eval_df = pd.DataFrame(readjsonl(task_path_ifeval))\n",
    "    instruction_id_list = ifeval_eval_df['instruction_id_list']\n",
    "    inst_list=[]\n",
    "    for i in instruction_id_list:\n",
    "        for j in i:\n",
    "            if j not in inst_list:\n",
    "                inst_list.append(j)\n",
    "    return inst_list\n",
    "\n",
    "# // Get all high level instructions\n",
    "def get_high_inst_list(task_path_ifeval=\"ifeval_simple\"):\n",
    "    task_path_ifeval=\"../data/\"+task_path_ifeval+\".jsonl\"\n",
    "    ifeval_eval_df = pd.DataFrame(readjsonl(task_path_ifeval))\n",
    "    instruction_id_list = ifeval_eval_df['instruction_id_list']\n",
    "    inst_list=[]\n",
    "    for i in instruction_id_list:\n",
    "        for j in i:\n",
    "            j = j.split(':')[0]\n",
    "            if j not in inst_list:\n",
    "                inst_list.append(j)\n",
    "    return inst_list\n",
    "\n",
    "# // Get all task type\n",
    "def get_task_list(task_path_ifeval=\"ifeval_simple\"):\n",
    "    task_path_ifeval=\"../data/\"+task_path_ifeval+\".jsonl\"\n",
    "    ifeval_eval_df = pd.DataFrame(readjsonl(task_path_ifeval))\n",
    "    prompt_df = ifeval_eval_df['prompt']\n",
    "    task_list=[]\n",
    "    tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "    for prompt in prompt_df:\n",
    "        task = tokenizer.tokenize(prompt)[0]\n",
    "        if task not in task_list:\n",
    "            task_list.append(task)\n",
    "    return task_list\n",
    "\n",
    "get_inst_list(), get_high_inst_list(), get_task_list(), len(get_task_list())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Probes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataModuleActIfevalSimple:\n",
    "    def __init__(self,\n",
    "                 ifeval_eval_path,\n",
    "                 inst_list, \n",
    "                 task_list,\n",
    "                 layer=13, \n",
    "                 target_token='last',\n",
    "                 center=True,\n",
    "                 scale=False,\n",
    "                 ):\n",
    "        self.layer=layer\n",
    "\n",
    "        # // Load data\n",
    "        ifeval_data_path = \"../data/ifeval_simple.jsonl\" \n",
    "        self.ifeval_data = self.load_response_df(ifeval_data_path)\n",
    "        ifeval_eval_df = self.load_response_df(os.path.join(ifeval_eval_path, 'eval_results_loose.jsonl')) # // <-- Please run the IFEval code for generating this file https://huggingface.co/datasets/google/IFEval\n",
    "        \n",
    "\n",
    "        # // Select index by inst\n",
    "        inst_ind = []\n",
    "        for i in range(len(ifeval_eval_df)):\n",
    "            if ifeval_eval_df.iloc[i]['instruction_id_list'][0] in inst_list:\n",
    "                inst_ind.append(i)\n",
    "\n",
    "        # // Select index by task\n",
    "        task_ind = []\n",
    "        tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "        for i in range(len(ifeval_eval_df)):\n",
    "            prompt = ifeval_eval_df.iloc[i]['prompt']\n",
    "            task = tokenizer.tokenize(prompt)[0]\n",
    "            if task in task_list:\n",
    "                task_ind.append(i)\n",
    "\n",
    "        # // Select index intersection\n",
    "        select_ind = list(set(inst_ind) & set(task_ind))\n",
    "\n",
    "        # // Load acts and labels\n",
    "        self.labels = torch.tensor(ifeval_eval_df['follow_all_instructions'])[select_ind]\n",
    "        self.labels = self.labels.float()\n",
    "        self.acts = self.collect_acts(ifeval_eval_path, layer=self.layer, target_token=target_token, device='cuda', center=center, scale=scale, index_list=select_ind)\n",
    "        self.acts = self.acts.float()\n",
    "        self.data={}\n",
    "        self.data = self.acts, self.labels\n",
    "        print('Saved layers: ', self.saved_layers)\n",
    "\n",
    "    def load_response_df(self, task_path, type='loose'):\n",
    "        response_df = pd.DataFrame(self.readjsonl(task_path))\n",
    "        return response_df\n",
    "    \n",
    "    def readjsonl(self, datapath):\n",
    "        res = []\n",
    "        with open(datapath, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f.readlines():\n",
    "                res.append(json.loads(line))\n",
    "        return res\n",
    "    \n",
    "    def load_pickle(self, filename: str):\n",
    "        with open(filename, \"rb\") as f:\n",
    "            return pickle.load(f)\n",
    "\n",
    "    def collect_acts(self, task_path, layer=13, target_token='last', device='cuda', center=True, scale=False, index_list=None):\n",
    "        \"\"\"\n",
    "        Collects activations from a dataset of statements, returns as a tensor of shape [n_activations, activation_dimension].\n",
    "        First token: [1, len_input, hidden_emb]\n",
    "        Last token: [1, 1, hidden_emb]\n",
    "        \"\"\"\n",
    "        act_path = os.path.join(task_path, \"activations\")\n",
    "        _num_act = len(os.listdir(act_path))\n",
    "        acts = []\n",
    "        print('num_act: ', _num_act)\n",
    "        for _idx in range(_num_act):\n",
    "            if index_list is not None and _idx in index_list:\n",
    "                act_file_name = os.path.join(act_path, f\"sample_{_idx}.pkl\")\n",
    "                act = self.load_pickle(act_file_name)       \n",
    "                self.saved_layers = act[f'output_token_{target_token}'].keys()\n",
    "                act = act[f'output_token_{target_token}'][f'layer_{layer}']\n",
    "                act = act[:,-1] # <-- last of the first token, no problem for last token --> [1, hidden_emb]\n",
    "                acts.append(act)\n",
    "        acts = torch.cat(acts, dim=0).to(device)\n",
    "        if center:\n",
    "            acts = acts - torch.mean(acts, dim=0)\n",
    "        if scale:\n",
    "            acts = acts / torch.std(acts, dim=0)\n",
    "        return acts\n",
    "\n",
    "\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LRProbe(t.nn.Module):\n",
    "    def __init__(self, d_in, binary_threshold=0.5, **kwargs):\n",
    "        super().__init__()\n",
    "        self.net = t.nn.Sequential(\n",
    "            t.nn.Linear(d_in, 1, bias=False),\n",
    "            t.nn.Sigmoid()\n",
    "        )\n",
    "        self.binary_threshold = binary_threshold\n",
    "\n",
    "    def forward(self, x, iid=None):\n",
    "        return self.net(x).squeeze(-1)\n",
    "\n",
    "    def pred(self, x, iid=None, binary_threshold=None):\n",
    "        binary_threshold = binary_threshold if binary_threshold is not None else self.binary_threshold\n",
    "        return (self(x)>binary_threshold).float()\n",
    "    \n",
    "    def probability(self, x, iid=None):\n",
    "        return self(x)\n",
    "    \n",
    "    def from_data(acts, labels, lr=0.001, weight_decay=0.1, epochs=1000, device='cpu', class_weight_one=None, **kwargs):\n",
    "        acts, labels = acts.to(device), labels.to(device)\n",
    "        probe = LRProbe(acts.shape[-1]).to(device)\n",
    "        \n",
    "        opt = t.optim.AdamW(probe.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        for _ in range(epochs):\n",
    "            opt.zero_grad()\n",
    "            if class_weight_one is not None:\n",
    "                class_weight = torch.ones_like(labels)\n",
    "                class_weight[labels>0] = class_weight_one\n",
    "                loss = t.nn.BCELoss(weight=class_weight)(probe(acts), labels)\n",
    "            else:\n",
    "                loss = t.nn.BCELoss()(probe(acts), labels)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "        \n",
    "        return probe\n",
    "\n",
    "    @property\n",
    "    def direction(self):\n",
    "        return self.net[0].weight.data[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task generalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LAYER=14\n",
    "MODEL='Llama-2-7b-chat-hf'\n",
    "TOKEN='first'\n",
    "\n",
    "task_path_ifeval = f\"./data/{MODEL}/ifeval_simple\"\n",
    "\n",
    "# // Seed\n",
    "roc_list=[]\n",
    "m_roc_list=[]\n",
    "seed_list = np.random.randint(0, 10000, 5)\n",
    "for seed in seed_list:\n",
    "    print(seed)\n",
    "\n",
    "    # // Select train and test task\n",
    "    task_list = np.array(get_task_list())\n",
    "    torch.manual_seed(seed)\n",
    "    split=0.8\n",
    "    train_ind_list = torch.randperm(len(task_list)) < int(split * len(task_list))\n",
    "    test_ind_list = ~train_ind_list\n",
    "    train_task_list = task_list[train_ind_list]\n",
    "    test_task_list = task_list[test_ind_list]\n",
    "\n",
    "    # // Use all instructions\n",
    "    inst_list = get_inst_list()\n",
    "\n",
    "    # // Get train data\n",
    "    train_dm = DataModuleActIfevalSimple(task_path_ifeval, inst_list, train_task_list, layer=LAYER, target_token=TOKEN, center=True, scale=True)\n",
    "    test_dm = DataModuleActIfevalSimple(task_path_ifeval, inst_list, test_task_list, layer=LAYER, target_token=TOKEN, center=True, scale=True)\n",
    "    train_acts, train_labels = train_dm.data\n",
    "    test_acts, test_labels = test_dm.data\n",
    "\n",
    "    # // Scale and Center\n",
    "    all_acts = torch.cat((train_acts, test_acts))\n",
    "    print(all_acts.shape)\n",
    "    train_acts = train_acts - torch.mean(train_acts, dim=0)\n",
    "    train_acts = train_acts / torch.std(train_acts, dim=0)\n",
    "    test_acts = test_acts - torch.mean(train_acts, dim=0)\n",
    "    test_acts = test_acts / torch.std(train_acts, dim=0)\n",
    "\n",
    "    # // Stat of test\n",
    "    succ = (test_labels==1).sum()\n",
    "    fail = (test_labels==0).sum()\n",
    "    print('succ: ', succ)\n",
    "    print('fail: ', fail)\n",
    "\n",
    "\n",
    "    # // Train probe\n",
    "    max_roc=0\n",
    "    probe = LRProbe.from_data(train_acts, train_labels, device='cuda', epochs=1000, binary_threshold=0.5)\n",
    "\n",
    "    # // Test\n",
    "    test_prob = probe.probability(test_acts).detach().cpu()\n",
    "    auroc = roc_auc_score(test_labels, test_prob)\n",
    "    roc_list.append(auroc)\n",
    "\n",
    "    print(LRProbe, ': ', auroc)\n",
    "    print()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intruction generalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LAYER=14\n",
    "MODEL='Llama-2-7b-chat-hf'\n",
    "TOKEN='first'\n",
    "\n",
    "task_path_ifeval = f\"../data/{MODEL}/ifeval_simple\"\n",
    "\n",
    "# // Make a dict for result\n",
    "inst_list = np.array(get_inst_list())\n",
    "re={}\n",
    "all_label={}\n",
    "all_pred={}\n",
    "for i in inst_list:\n",
    "    if i not in re.keys():\n",
    "        re[i]=[]\n",
    "        all_label[i]=[]\n",
    "        all_pred[i]=[]\n",
    "roc_list=[]\n",
    "total_pred=[]\n",
    "total_label=[]\n",
    "\n",
    "# // Use all task\n",
    "task_list = get_task_list()\n",
    "\n",
    "# // Select train and test inst\n",
    "inst_list = np.array(get_inst_list())\n",
    "\n",
    "keyword_list = [\\\n",
    "    'keywords:frequency',\n",
    "    'keywords:forbidden_words',\n",
    "    'keywords:existence',\n",
    "    'detectable_content:number_placeholders',\n",
    "    \"startend:end_checker\"\n",
    "    ]\n",
    "\n",
    "final={}\n",
    "for inst in inst_list:\n",
    "    final[inst]=[]\n",
    "\n",
    "for inst in inst_list:\n",
    "\n",
    "    # // Leave one out\n",
    "    train_inst_list = [i for i in keyword_list if i != inst]\n",
    "    test_inst_list = [inst]\n",
    "    print(train_inst_list)\n",
    "    print(test_inst_list)\n",
    "\n",
    "    # // Get train data\n",
    "    train_dm = DataModuleActIfevalSimple(task_path_ifeval, train_inst_list, task_list, layer=LAYER, target_token=TOKEN, center=True, scale=True)\n",
    "    test_dm = DataModuleActIfevalSimple(task_path_ifeval, test_inst_list, task_list, layer=LAYER, target_token=TOKEN, center=True, scale=True)\n",
    "    train_acts, train_labels = train_dm.data\n",
    "    test_acts, test_labels = test_dm.data\n",
    "\n",
    "    # // Scale and Center\n",
    "    all_acts = torch.cat((train_acts, test_acts))\n",
    "    print(all_acts.shape)\n",
    "    train_acts = train_acts - torch.mean(train_acts, dim=0)\n",
    "    train_acts = train_acts / torch.std(train_acts, dim=0)\n",
    "    test_acts = test_acts - torch.mean(train_acts, dim=0)\n",
    "    test_acts = test_acts / torch.std(train_acts, dim=0)\n",
    "\n",
    "    # // Stat of test\n",
    "    succ = (test_labels==1).sum()\n",
    "    fail = (test_labels==0).sum()\n",
    "    print('te_succ: ', succ)\n",
    "    print('te_fail: ', fail)\n",
    "\n",
    "    # // Stat of train\n",
    "    tr_succ = (train_labels==1).sum()\n",
    "    tr_fail = (train_labels==0).sum()\n",
    "    print('tr_succ: ', tr_succ)\n",
    "    print('tr_fail: ', tr_fail)\n",
    "    tr_class_weight = tr_succ/tr_fail\n",
    "\n",
    "    # // exception\n",
    "    if succ<1 or fail<1:\n",
    "        continue\n",
    "\n",
    "\n",
    "    # // Train probe\n",
    "    probe = LRProbe.from_data(train_acts, train_labels, device='cuda', epochs=1000, binary_threshold=0.5, class_weight_one=None)\n",
    "\n",
    "    # // Test\n",
    "    test_prob = probe.probability(test_acts).detach().cpu()\n",
    "    auroc = roc_auc_score(test_labels, test_prob)\n",
    "\n",
    "    print(LRProbe, ': ', auroc)\n",
    "    print()\n",
    "    \n",
    "    # // save\n",
    "    roc_list.append(auroc)\n",
    "    re[inst].append(auroc)\n",
    "    all_label[inst].append(test_labels)\n",
    "    all_pred[inst].append(test_prob)\n",
    "    total_label.append(test_labels)\n",
    "    total_pred.append(test_prob)\n",
    "\n",
    "for key in all_pred.keys():\n",
    "    if len(all_pred[key])>0:\n",
    "        print(key)\n",
    "        label = np.concatenate(all_label[key])\n",
    "        pred = np.concatenate(all_pred[key])\n",
    "        final[key].append(roc_auc_score(label, pred ))\n",
    "\n",
    "# // Compute all auc total\n",
    "label = np.concatenate(total_label)\n",
    "pred = np.concatenate(total_pred)\n",
    "total_auroc = roc_auc_score(label, pred)\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
